{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868f33b6-5850-4963-bea6-05b9c005ecaf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668b1181-7004-43b4-be8c-e2eb4fb30d4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers import InputExample, LoggingHandler, util\n",
    "from sentence_transformers.cross_encoder import CrossEncoder\n",
    "from sentence_transformers.cross_encoder.evaluation import CERerankingEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cdc4a242-84e1-4c72-8613-c936649e0d1f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/huggingface-cli\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "             ^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/commands/huggingface_cli.py\", line 49, in main\n",
      "    service.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/commands/user.py\", line 98, in run\n",
      "    login(token=self.args.token, add_to_git_credential=self.args.add_to_git_credential)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/_login.py\", line 109, in login\n",
      "    _login(token, add_to_git_credential=add_to_git_credential, write_permission=write_permission)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/_login.py\", line 305, in _login\n",
      "    raise ValueError(\"Invalid token passed!\")\n",
      "ValueError: Invalid token passed!\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token {token}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3586f8b-ed9f-463f-abed-03c3a00fbed6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\", level=logging.INFO, handlers=[LoggingHandler()]\n",
    ")\n",
    "\n",
    "\n",
    "# Define paths and model configurations\n",
    "data_folder = \"arabic-msmarco-data\"\n",
    "os.makedirs(data_folder, exist_ok=True)\n",
    "model_name = \"Omartificial-Intelligence-Space/Arabic-Triplet-Matryoshka-V2\"\n",
    "train_batch_size = 8\n",
    "num_epochs = 1\n",
    "model_save_path = (\n",
    "    \"output/training_matroV2_1N_arabic_msmarco_cross-encoder-\"\n",
    "    + model_name.replace(\"/\", \"-\")\n",
    "    + \"-\"\n",
    "    + datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0db10394-d3df-4901-8b61-8b4af453fe01",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-07 02:44:05 - Loading training triplets dataset\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e82fe1132308489aa6d92f9707854fe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/637 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32aeba6081e9454d93f2d399f9fadb71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/541M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Omartificial-Intelligence-Space/Arabic-Triplet-Matryoshka-V2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5094407afef49f88985afc2b96ddcd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.82k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe5e9c1d79e248ebad0560ade28d0454",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/761k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "038d3f71b34a4935bb28226ec064d7a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71d2581c8bb6470b9ef5582ce8bb5f85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/695 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-07 02:44:20 - Use pytorch device: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the Arabic MSMARCO triplet dataset for training samples\n",
    "logging.info(\"Loading training triplets dataset\")\n",
    "triplet_dataset = load_dataset(\"oddadmix/arabic-triplets\", split=\"train\")\n",
    "\n",
    "\n",
    "# Set positive-to-negative sample ratio\n",
    "pos_neg_ratio = 1\n",
    "max_train_samples = int(1e6)\n",
    "max_dev_samples = 200  # Number of evaluation samples\n",
    "\n",
    "\n",
    "# Set up model with a continuous score output\n",
    "model = CrossEncoder(model_name, num_labels=1, max_length=512)\n",
    "\n",
    "\n",
    "# Prepare training and development samples\n",
    "train_samples = []\n",
    "dev_samples = {}\n",
    "\n",
    "\n",
    "# Create training samples with positive-to-negative ratio\n",
    "for i, example in enumerate(triplet_dataset):\n",
    "    query = example[\"question\"]\n",
    "    positive_passage = example[\"positive\"]\n",
    "    negative_passage = example[\"negative\"]\n",
    "\n",
    "\n",
    "    # Add the positive example\n",
    "    train_samples.append(InputExample(texts=[query, positive_passage], label=1))\n",
    "\n",
    "\n",
    "    # Add negative examples (according to pos_neg_ratio)\n",
    "    for _ in range(pos_neg_ratio):\n",
    "        train_samples.append(InputExample(texts=[query, negative_passage], label=0))\n",
    "\n",
    "\n",
    "    # Collect some samples for development evaluation\n",
    "    if len(dev_samples) < max_dev_samples:\n",
    "        if query not in dev_samples:\n",
    "            dev_samples[query] = {\"query\": query, \"positive\": set(), \"negative\": set()}\n",
    "        dev_samples[query][\"positive\"].add(positive_passage)\n",
    "        if len(dev_samples[query][\"negative\"]) < pos_neg_ratio:\n",
    "            dev_samples[query][\"negative\"].add(negative_passage)\n",
    "\n",
    "\n",
    "    # Limit the number of training samples if necessary\n",
    "    if len(train_samples) >= max_train_samples:\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63f8894-624f-4502-be37-4709535ad6d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-07 02:45:01 - Warmup-steps: 5000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eb4cdd8b7fc47089030fe9ca7307a9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "476d269b6e07468c8dd9c9dc5730dfe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/4520 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_batch_size = 8\n",
    "num_epochs = 1\n",
    "\n",
    "\n",
    "# Create a DataLoader to load training samples\n",
    "train_dataloader = DataLoader(train_samples, shuffle=True, batch_size=train_batch_size)\n",
    "\n",
    "\n",
    "# Set up evaluator with development samples for monitoring performance\n",
    "evaluator = CERerankingEvaluator(dev_samples, name=\"train-eval\")\n",
    "\n",
    "\n",
    "# Configure the training\n",
    "warmup_steps = 5000  # Adjust based on dataset size\n",
    "logging.info(f\"Warmup-steps: {warmup_steps}\")\n",
    "\n",
    "\n",
    "# Train the model with evaluation at intervals\n",
    "model.fit(\n",
    "    train_dataloader=train_dataloader,\n",
    "    evaluator=evaluator,\n",
    "    epochs=num_epochs,\n",
    "    evaluation_steps=10000,  # Adjust evaluation frequency as needed\n",
    "    warmup_steps=warmup_steps,\n",
    "    output_path=model_save_path,\n",
    "    use_amp=True,  # Use automatic mixed precision for faster training,\n",
    ")\n",
    "\n",
    "\n",
    "# Save the latest model\n",
    "model.save(model_save_path + \"-latest\")\n",
    "\n",
    "## if need to push to the huggingface repo\n",
    "#model.model.push_to_hub(\"oddadmix/arabic-reranker\")\n",
    "#model.tokenizer.push_to_hub(\"oddadmix/arabic-reranker\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971cc567-08b9-4542-9567-c1b2e7e65590",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##Evaluation\n",
    "\n",
    "from sentence_transformers import CrossEncoder\n",
    "model = CrossEncoder(model_save_path + \"-latest\", max_length=512)\n",
    "\n",
    "\n",
    "Query = 'كيف يمكن استخدام التعلم العميق في معالجة الصور الطبية؟'\n",
    "Paragraph1 = 'التعلم العميق يساعد في تحليل الصور الطبية وتشخيص الأمراض'\n",
    "Paragraph2 = 'الذكاء الاصطناعي يستخدم في تحسين الإنتاجية في الصناعات'\n",
    "Paragraph3 = 'التعلم العميق يساعد في تحليل الصور الطبية '\n",
    "\n",
    "scores = model.predict([(Query, Paragraph1), (Query, Paragraph2), (Query, Paragraph3)])\n",
    "\n",
    "print(scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
